# MacJarvis 性能优化建议

> **文档版本**: v1.0
> **创建日期**: 2026-01-27
> **优化目标**: 提升系统响应速度和并发处理能力

---

## 目录

1. [性能基准测试](#1-性能基准测试)
2. [响应时间优化](#2-响应时间优化)
3. [并发处理优化](#3-并发处理优化)
4. [数据库性能优化](#4-数据库性能优化)
5. [缓存策略优化](#5-缓存策略优化)
6. [网络传输优化](#6-网络传输优化)
7. [资源利用优化](#7-资源利用优化)
8. [前端性能优化](#8-前端性能优化)
9. [监控与调优](#9-监控与调优)

---

## 1. 性能基准测试

### 1.1 关键性能指标 (KPI)

| 指标 | 当前值 | 目标值 | 优先级 |
|------|--------|--------|--------|
| **API 响应时间** | 未测量 | P50: <200ms<br>P95: <500ms<br>P99: <1s | 高 |
| **LLM 首字响应** | ~2-3s | <1s | 高 |
| **流式响应延迟** | ~100ms | <50ms | 中 |
| **并发用户数** | 未知 | 1000+ | 高 |
| **工具执行时间** | 1-30s | <5s (P95) | 中 |
| **数据库查询** | 未测量 | <50ms (P95) | 高 |
| **前端加载时间** | 未测量 | <2s (First Contentful Paint) | 中 |
| **内存使用** | 未测量 | <512MB per instance | 中 |
| **CPU 使用率** | 未测量 | <70% (avg) | 低 |

### 1.2 性能测试框架

```python
# performance_test.py
import asyncio
import time
from statistics import mean, median, quantiles
from dataclasses import dataclass

@dataclass
class PerformanceMetrics:
    total_requests: int
    successful: int
    failed: int
    response_times: list[float]
    errors: list[str]

    @property
    def p50(self) -> float:
        return median(self.response_times)

    @property
    def p95(self) -> float:
        return quantiles(self.response_times, n=20)[18]  # 95th percentile

    @property
    def p99(self) -> float:
        return quantiles(self.response_times, n=100)[98]

    @property
    def mean(self) -> float:
        return mean(self.response_times)

class PerformanceTester:
    def __init__(self, base_url: str):
        self.base_url = base_url
        self.client = httpx.AsyncClient()

    async def load_test(
        self,
        endpoint: str,
        payload: dict,
        num_requests: int = 1000,
        concurrency: int = 10
    ) -> PerformanceMetrics:
        """负载测试"""
        semaphore = asyncio.Semaphore(concurrency)
        response_times = []
        errors = []
        successful = 0

        async def single_request():
            async with semaphore:
                start = time.time()
                try:
                    response = await self.client.post(
                        f"{self.base_url}{endpoint}",
                        json=payload,
                        timeout=30
                    )
                    elapsed = time.time() - start
                    response_times.append(elapsed)

                    if response.status_code == 200:
                        nonlocal successful
                        successful += 1
                    else:
                        errors.append(f"Status {response.status_code}")
                except Exception as e:
                    errors.append(str(e))

        # 并发执行
        await asyncio.gather(*[
            single_request() for _ in range(num_requests)
        ])

        return PerformanceMetrics(
            total_requests=num_requests,
            successful=successful,
            failed=len(errors),
            response_times=response_times,
            errors=errors
        )

    async def benchmark_api(self):
        """API 基准测试"""
        print("=== API Performance Benchmark ===\n")

        # 1. 健康检查
        metrics = await self.load_test(
            "/health",
            {},
            num_requests=1000,
            concurrency=50
        )
        print(f"Health Check:")
        print(f"  P50: {metrics.p50*1000:.2f}ms")
        print(f"  P95: {metrics.p95*1000:.2f}ms")
        print(f"  P99: {metrics.p99*1000:.2f}ms")
        print(f"  Success Rate: {metrics.successful/metrics.total_requests*100:.2f}%\n")

        # 2. 会话创建
        metrics = await self.load_test(
            "/api/sessions",
            {"title": "Test Session"},
            num_requests=500,
            concurrency=20
        )
        print(f"Session Creation:")
        print(f"  P50: {metrics.p50*1000:.2f}ms")
        print(f"  P95: {metrics.p95*1000:.2f}ms\n")

        # 3. 聊天 API（非流式）
        metrics = await self.load_test(
            "/api/agent/chat/sync",
            {"message": "hello", "session_id": "test_session"},
            num_requests=100,
            concurrency=10
        )
        print(f"Chat API (sync):")
        print(f"  P50: {metrics.p50:.2f}s")
        print(f"  P95: {metrics.p95:.2f}s\n")

# 使用示例
tester = PerformanceTester("http://localhost:8000")
await tester.benchmark_api()
```

---

## 2. 响应时间优化

### 2.1 减少 LLM 首字响应时间

**问题**: LLM API 调用需要 2-3 秒才开始返回

**优化方案**:

#### 方案 1: 请求预热

```python
class LLMClientWithWarmup:
    def __init__(self, client: OpenAIClient):
        self.client = client
        self.warmup_done = False

    async def warmup(self):
        """预热 LLM 连接"""
        if not self.warmup_done:
            # 发送一个简单请求预热连接
            await self.client.chat(
                messages=[{"role": "user", "content": "hi"}],
                max_tokens=1
            )
            self.warmup_done = True

    async def chat(self, **kwargs):
        """确保连接已预热"""
        if not self.warmup_done:
            await self.warmup()
        return await self.client.chat(**kwargs)
```

#### 方案 2: 连接池复用

```python
import httpx

class OptimizedHTTPClient:
    def __init__(self):
        # 使用连接池
        self.client = httpx.AsyncClient(
            timeout=httpx.Timeout(60.0, connect=5.0),
            limits=httpx.Limits(
                max_keepalive_connections=20,
                max_connections=100,
                keepalive_expiry=30.0
            ),
            http2=True  # 启用 HTTP/2
        )

    async def post(self, url: str, **kwargs):
        return await self.client.post(url, **kwargs)
```

#### 方案 3: 流式响应优化

```python
async def stream_chat_optimized(
    messages: list[dict],
    llm_client: LLMClient
) -> AsyncIterator[str]:
    """优化的流式响应"""
    buffer = []
    buffer_size = 5  # 缓冲 5 个 token 后再发送

    async for chunk in llm_client.stream(messages):
        if chunk.type == "content":
            buffer.append(chunk.content)

            # 达到缓冲大小或遇到标点符号，立即发送
            if len(buffer) >= buffer_size or chunk.content in ".!?。！？":
                yield "".join(buffer)
                buffer = []

    # 发送剩余内容
    if buffer:
        yield "".join(buffer)
```

---

### 2.2 工具执行并行化

**问题**: 当前工具顺序执行，浪费时间

**优化方案**: 并行执行独立工具

```python
class ParallelToolExecutor:
    """并行工具执行器"""

    async def execute_batch(
        self,
        tool_calls: list[dict],
        registry: ToolRegistry
    ) -> list[dict]:
        """并行执行多个工具调用"""
        # 分析依赖关系
        dependency_graph = self._build_dependency_graph(tool_calls)

        # 按层级分组
        levels = self._topological_sort(dependency_graph)

        results = {}

        # 逐层执行（层内并行）
        for level in levels:
            tasks = []
            for tool_call in level:
                # 解析参数中的依赖
                args = self._resolve_dependencies(
                    tool_call["args"],
                    results
                )
                tasks.append(
                    self._execute_single(
                        tool_call["id"],
                        tool_call["name"],
                        args,
                        registry
                    )
                )

            # 并行执行当前层
            level_results = await asyncio.gather(*tasks)

            # 保存结果
            for tool_call, result in zip(level, level_results):
                results[tool_call["id"]] = result

        return [results[tc["id"]] for tc in tool_calls]

    def _build_dependency_graph(
        self,
        tool_calls: list[dict]
    ) -> dict[str, list[str]]:
        """构建依赖图"""
        graph = {tc["id"]: [] for tc in tool_calls}

        for tc in tool_calls:
            # 检查参数中是否引用其他工具的结果
            deps = self._find_dependencies(tc["args"])
            graph[tc["id"]].extend(deps)

        return graph

    def _topological_sort(
        self,
        graph: dict[str, list[str]]
    ) -> list[list[dict]]:
        """拓扑排序（分层）"""
        # Kahn 算法
        in_degree = {node: 0 for node in graph}
        for deps in graph.values():
            for dep in deps:
                in_degree[dep] += 1

        levels = []
        current_level = [
            node for node, degree in in_degree.items()
            if degree == 0
        ]

        while current_level:
            levels.append(current_level)
            next_level = []

            for node in current_level:
                for neighbor in graph.get(node, []):
                    in_degree[neighbor] -= 1
                    if in_degree[neighbor] == 0:
                        next_level.append(neighbor)

            current_level = next_level

        return levels

    async def _execute_single(
        self,
        call_id: str,
        tool_name: str,
        args: dict,
        registry: ToolRegistry
    ) -> dict:
        """执行单个工具"""
        try:
            result = await registry.execute(tool_name, args)
            return {"id": call_id, "result": result}
        except Exception as e:
            return {"id": call_id, "error": str(e)}
```

---

### 2.3 智能预取

预测用户可能需要的数据并提前加载：

```python
class PrefetchEngine:
    """数据预取引擎"""

    def __init__(self, redis: Redis, db: Database):
        self.redis = redis
        self.db = db

    async def prefetch_session_data(self, user_id: str):
        """预取用户会话数据"""
        # 1. 获取用户最近的会话列表
        sessions = await self.db.query(
            "SELECT id FROM sessions WHERE user_id = ? ORDER BY updated_at DESC LIMIT 5",
            user_id
        )

        # 2. 并行预取会话详情
        await asyncio.gather(*[
            self._prefetch_session_messages(s["id"])
            for s in sessions
        ])

    async def _prefetch_session_messages(self, session_id: str):
        """预取会话消息"""
        # 检查缓存
        cache_key = f"session:{session_id}:messages"
        if await self.redis.exists(cache_key):
            return

        # 从数据库加载
        messages = await self.db.sessions.find_one(
            {"_id": session_id},
            {"messages": 1}
        )

        # 写入缓存
        await self.redis.setex(
            cache_key,
            600,  # 10分钟
            json.dumps(messages)
        )

    async def predict_and_prefetch(
        self,
        user_id: str,
        current_query: str
    ):
        """预测并预取可能需要的数据"""
        # 使用简单规则或机器学习预测
        predictions = await self._predict_next_tools(current_query)

        for tool_name in predictions:
            # 预执行工具（如果是幂等工具）
            if tool_name in ["system_info", "disk_usage"]:
                asyncio.create_task(
                    self._prefetch_tool_result(tool_name)
                )

    async def _prefetch_tool_result(self, tool_name: str):
        """预取工具结果"""
        cache_key = f"prefetch:tool:{tool_name}"

        # 执行工具
        result = await tool_registry.execute(tool_name, {})

        # 缓存结果（短期）
        await self.redis.setex(cache_key, 60, json.dumps(result))
```

---

## 3. 并发处理优化

### 3.1 异步 I/O 优化

**全面使用异步**:

```python
# ❌ 阻塞 I/O
def sync_database_query():
    conn = psycopg2.connect(...)
    cursor = conn.cursor()
    cursor.execute("SELECT * FROM users")
    return cursor.fetchall()

# ✅ 异步 I/O
async def async_database_query():
    async with asyncpg.connect(...) as conn:
        return await conn.fetch("SELECT * FROM users")
```

### 3.2 任务队列与工作池

```python
class WorkerPool:
    """异步工作池"""

    def __init__(self, max_workers: int = 10):
        self.max_workers = max_workers
        self.semaphore = asyncio.Semaphore(max_workers)
        self.queue = asyncio.Queue()
        self.workers = []

    async def start(self):
        """启动工作池"""
        self.workers = [
            asyncio.create_task(self._worker())
            for _ in range(self.max_workers)
        ]

    async def _worker(self):
        """工作协程"""
        while True:
            try:
                # 从队列获取任务
                task_func, args, kwargs, future = await self.queue.get()

                # 执行任务
                try:
                    result = await task_func(*args, **kwargs)
                    future.set_result(result)
                except Exception as e:
                    future.set_exception(e)
                finally:
                    self.queue.task_done()

            except asyncio.CancelledError:
                break

    async def submit(
        self,
        func: Callable,
        *args,
        **kwargs
    ) -> Any:
        """提交任务"""
        future = asyncio.Future()
        await self.queue.put((func, args, kwargs, future))
        return await future

    async def shutdown(self):
        """关闭工作池"""
        await self.queue.join()
        for worker in self.workers:
            worker.cancel()

# 使用示例
pool = WorkerPool(max_workers=20)
await pool.start()

# 并发执行 100 个任务
results = await asyncio.gather(*[
    pool.submit(some_async_function, arg)
    for arg in range(100)
])
```

### 3.3 请求限流与背压

```python
from asyncio import Semaphore

class RateLimiter:
    """速率限制器"""

    def __init__(self, rate: int, per: float = 1.0):
        """
        rate: 每 per 秒允许的请求数
        per: 时间窗口（秒）
        """
        self.rate = rate
        self.per = per
        self.allowance = rate
        self.last_check = time.time()
        self.lock = asyncio.Lock()

    async def acquire(self):
        """获取许可"""
        async with self.lock:
            current = time.time()
            time_passed = current - self.last_check
            self.last_check = current

            # 补充令牌
            self.allowance += time_passed * (self.rate / self.per)
            if self.allowance > self.rate:
                self.allowance = self.rate

            # 消耗令牌
            if self.allowance < 1.0:
                # 等待
                sleep_time = (1.0 - self.allowance) * (self.per / self.rate)
                await asyncio.sleep(sleep_time)
                self.allowance = 0.0
            else:
                self.allowance -= 1.0

# 应用到 API
llm_limiter = RateLimiter(rate=10, per=1.0)  # 10 req/s

async def call_llm_with_limit(**kwargs):
    await llm_limiter.acquire()
    return await llm_client.chat(**kwargs)
```

---

## 4. 数据库性能优化

### 4.1 索引优化

```sql
-- PostgreSQL 索引

-- 用户查询优化
CREATE INDEX idx_users_email ON users(email);
CREATE INDEX idx_users_created_at ON users(created_at DESC);

-- 会话查询优化
CREATE INDEX idx_user_subscriptions_user_id_status
ON user_subscriptions(user_id, status)
WHERE status = 'active';

-- API 使用统计
CREATE INDEX idx_api_usage_user_created
ON api_usage(user_id, created_at DESC);

-- 部分索引（只索引活跃用户）
CREATE INDEX idx_active_users
ON users(id)
WHERE is_active = TRUE AND is_verified = TRUE;

-- 表达式索引
CREATE INDEX idx_users_email_lower
ON users(LOWER(email));
```

```javascript
// MongoDB 索引

// 会话查询
db.sessions.createIndex(
  { user_id: 1, updated_at: -1 },
  { background: true }
);

// 复合索引
db.sessions.createIndex(
  { user_id: 1, "metadata.is_archived": 1, updated_at: -1 },
  { background: true }
);

// 文本搜索索引
db.sessions.createIndex(
  { title: "text", "messages.content": "text" },
  { background: true }
);

// TTL 索引（自动过期）
db.temp_data.createIndex(
  { created_at: 1 },
  { expireAfterSeconds: 3600, background: true }
);

// 向量搜索索引
db.episodes.createIndex(
  {
    embedding: "vectorSearch"
  },
  {
    name: "episode_embedding_index",
    vectorSearchOptions: {
      numDimensions: 1536,
      similarity: "cosine"
    }
  }
);
```

### 4.2 查询优化

#### 批量查询

```python
# ❌ N+1 查询问题
async def get_users_with_sessions_bad(user_ids: list[str]):
    users = []
    for user_id in user_ids:
        user = await db.users.find_one({"_id": user_id})
        sessions = await db.sessions.find({"user_id": user_id}).to_list(10)
        user["sessions"] = sessions
        users.append(user)
    return users

# ✅ 批量查询
async def get_users_with_sessions_good(user_ids: list[str]):
    # 1. 批量获取用户
    users = await db.users.find({"_id": {"$in": user_ids}}).to_list(None)

    # 2. 批量获取会话
    sessions = await db.sessions.find(
        {"user_id": {"$in": user_ids}}
    ).to_list(None)

    # 3. 内存中组装
    sessions_by_user = {}
    for session in sessions:
        sessions_by_user.setdefault(session["user_id"], []).append(session)

    for user in users:
        user["sessions"] = sessions_by_user.get(user["_id"], [])

    return users
```

#### 投影优化

```python
# ❌ 查询所有字段
sessions = await db.sessions.find({"user_id": user_id}).to_list(None)

# ✅ 只查询需要的字段
sessions = await db.sessions.find(
    {"user_id": user_id},
    {"_id": 1, "title": 1, "updated_at": 1}  # 投影
).to_list(None)
```

#### 分页优化

```python
# ❌ 使用 OFFSET（大偏移量性能差）
sessions = await db.sessions.find({"user_id": user_id}) \
    .skip(1000) \
    .limit(20) \
    .to_list(20)

# ✅ 使用游标分页
async def get_sessions_cursor(
    user_id: str,
    last_id: str | None = None,
    limit: int = 20
):
    query = {"user_id": user_id}
    if last_id:
        query["_id"] = {"$gt": last_id}

    sessions = await db.sessions.find(query) \
        .sort("_id", 1) \
        .limit(limit) \
        .to_list(limit)

    return sessions
```

### 4.3 连接池优化

```python
# PostgreSQL 连接池
from asyncpg import create_pool

pool = await create_pool(
    host='localhost',
    port=5432,
    user='user',
    password='password',
    database='macjarvis',
    min_size=10,         # 最小连接数
    max_size=20,         # 最大连接数
    max_queries=50000,   # 每个连接最大查询数
    max_inactive_connection_lifetime=300  # 5分钟
)

# MongoDB 连接池
from motor.motor_asyncio import AsyncIOMotorClient

client = AsyncIOMotorClient(
    "mongodb://localhost:27017",
    maxPoolSize=50,
    minPoolSize=10,
    maxIdleTimeMS=30000,
    waitQueueTimeoutMS=5000
)
```

---

## 5. 缓存策略优化

### 5.1 缓存预热

```python
class CacheWarmer:
    """缓存预热器"""

    async def warmup_on_startup(self):
        """启动时预热缓存"""
        print("Warming up cache...")

        # 1. 预热常用配置
        await self._warmup_config()

        # 2. 预热热门数据
        await self._warmup_hot_data()

        # 3. 预热工具结果（幂等工具）
        await self._warmup_tools()

        print("Cache warmup complete")

    async def _warmup_config(self):
        """预热配置数据"""
        configs = await db.query("SELECT * FROM configs")
        for config in configs:
            await redis.setex(
                f"config:{config['key']}",
                3600,
                config['value']
            )

    async def _warmup_hot_data(self):
        """预热热门数据"""
        # 获取活跃用户
        active_users = await db.query("""
            SELECT id FROM users
            WHERE last_login_at > NOW() - INTERVAL '1 day'
            LIMIT 1000
        """)

        # 预热用户数据
        for user in active_users:
            user_data = await db.query(
                "SELECT * FROM users WHERE id = ?",
                user["id"]
            )
            await redis.setex(
                f"user:{user['id']}",
                3600,
                json.dumps(user_data)
            )

    async def _warmup_tools(self):
        """预热工具结果"""
        # 幂等工具可以预执行
        idempotent_tools = ["system_info", "list_applications"]

        for tool_name in idempotent_tools:
            try:
                result = await tool_registry.execute(tool_name, {})
                await redis.setex(
                    f"tool_cache:{tool_name}:{}",
                    300,  # 5分钟
                    json.dumps(result)
                )
            except Exception as e:
                print(f"Failed to warmup {tool_name}: {e}")
```

### 5.2 缓存失效策略

```python
class CacheInvalidator:
    """缓存失效管理"""

    async def invalidate_user_cache(self, user_id: str):
        """失效用户相关缓存"""
        pattern_keys = [
            f"user:{user_id}",
            f"user:{user_id}:*",
            f"sessions:{user_id}:*"
        ]

        for pattern in pattern_keys:
            if "*" in pattern:
                # 批量删除
                keys = await redis.keys(pattern)
                if keys:
                    await redis.delete(*keys)
            else:
                await redis.delete(pattern)

    async def invalidate_on_write(
        self,
        table: str,
        record_id: str
    ):
        """写操作时自动失效缓存"""
        # 根据表名确定缓存 key
        cache_patterns = {
            "users": [f"user:{record_id}"],
            "sessions": [
                f"session:{record_id}",
                f"user:*:sessions"
            ],
            "messages": [
                f"session:*:messages",
                f"message:{record_id}"
            ]
        }

        patterns = cache_patterns.get(table, [])
        for pattern in patterns:
            await self.invalidate_pattern(pattern)
```

---

## 6. 网络传输优化

### 6.1 压缩

```python
from fastapi import FastAPI
from fastapi.middleware.gzip import GZipMiddleware

app = FastAPI()

# 启用 GZIP 压缩
app.add_middleware(
    GZipMiddleware,
    minimum_size=1000,  # 大于 1KB 才压缩
    compresslevel=6     # 压缩级别 1-9
)

# 使用 Brotli（压缩率更高）
from brotli_asgi import BrotliMiddleware

app.add_middleware(
    BrotliMiddleware,
    minimum_size=1000,
    quality=4  # 压缩质量 0-11
)
```

### 6.2 HTTP/2 和连接复用

```nginx
# Nginx 配置
server {
    listen 443 ssl http2;  # 启用 HTTP/2
    server_name macjarvis.com;

    # SSL 配置
    ssl_certificate /path/to/cert.pem;
    ssl_certificate_key /path/to/key.pem;
    ssl_protocols TLSv1.2 TLSv1.3;

    # HTTP/2 推送
    location / {
        http2_push /static/app.css;
        http2_push /static/app.js;
        root /var/www/html;
    }

    # 连接保活
    keepalive_timeout 65;
    keepalive_requests 100;
}
```

### 6.3 CDN 加速

```python
# 使用 CDN 加速静态资源
CDN_BASE_URL = "https://cdn.macjarvis.com"

def get_static_url(path: str) -> str:
    """获取静态资源 URL"""
    if settings.USE_CDN:
        return f"{CDN_BASE_URL}/{path}"
    return f"/static/{path}"

# 前端配置
const API_URL = process.env.NODE_ENV === 'production'
  ? 'https://api.macjarvis.com'
  : 'http://localhost:8000';

const STATIC_URL = process.env.NODE_ENV === 'production'
  ? 'https://cdn.macjarvis.com'
  : '/static';
```

---

## 7. 资源利用优化

### 7.1 内存优化

```python
# 使用生成器减少内存占用
def read_large_file_bad(path: str) -> list[str]:
    """❌ 一次性加载到内存"""
    with open(path) as f:
        return f.readlines()  # 占用大量内存

def read_large_file_good(path: str) -> Iterator[str]:
    """✅ 使用生成器逐行读取"""
    with open(path) as f:
        for line in f:
            yield line.strip()

# 及时释放大对象
async def process_large_data():
    data = await load_large_dataset()

    # 处理数据
    result = process(data)

    # 显式删除，释放内存
    del data
    gc.collect()

    return result
```

### 7.2 CPU 优化

#### 使用 Process Pool 处理 CPU 密集型任务

```python
from concurrent.futures import ProcessPoolExecutor

class CPUIntensiveTaskExecutor:
    def __init__(self, max_workers: int = None):
        self.executor = ProcessPoolExecutor(max_workers=max_workers)

    async def run_cpu_task(self, func: Callable, *args):
        """在进程池中运行 CPU 密集型任务"""
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(
            self.executor,
            func,
            *args
        )

# 使用示例
executor = CPUIntensiveTaskExecutor(max_workers=4)

# CPU 密集型任务（如数据处理、加密）
def heavy_computation(data: list[int]) -> int:
    return sum(x**2 for x in data)

result = await executor.run_cpu_task(
    heavy_computation,
    list(range(1000000))
)
```

---

## 8. 前端性能优化

### 8.1 代码分割与懒加载

```typescript
// React 代码分割
import { lazy, Suspense } from 'react';

// 懒加载组件
const ChatInterface = lazy(() => import('./components/ChatInterface'));
const Settings = lazy(() => import('./components/Settings'));

function App() {
  return (
    <Suspense fallback={<LoadingSpinner />}>
      <Routes>
        <Route path="/" element={<ChatInterface />} />
        <Route path="/settings" element={<Settings />} />
      </Routes>
    </Suspense>
  );
}

// Vite 配置优化
export default defineConfig({
  build: {
    rollupOptions: {
      output: {
        manualChunks: {
          vendor: ['react', 'react-dom'],
          markdown: ['react-markdown', 'remark-gfm'],
        },
      },
    },
  },
});
```

### 8.2 虚拟滚动

```typescript
import { FixedSizeList } from 'react-window';

function MessageList({ messages }: { messages: Message[] }) {
  const Row = ({ index, style }: { index: number; style: React.CSSProperties }) => (
    <div style={style}>
      <ChatMessage message={messages[index]} />
    </div>
  );

  return (
    <FixedSizeList
      height={600}
      itemCount={messages.length}
      itemSize={80}
      width="100%"
    >
      {Row}
    </FixedSizeList>
  );
}
```

### 8.3 防抖与节流

```typescript
import { debounce, throttle } from 'lodash-es';

// 防抖：搜索输入
const handleSearchDebounced = debounce((query: string) => {
  performSearch(query);
}, 300);

// 节流：滚动事件
const handleScrollThrottled = throttle(() => {
  loadMoreMessages();
}, 200);
```

---

## 9. 监控与调优

### 9.1 性能监控

```python
from prometheus_client import Counter, Histogram, Gauge
import time

# 定义指标
api_request_duration = Histogram(
    'api_request_duration_seconds',
    'API request duration',
    ['endpoint', 'method']
)

api_request_total = Counter(
    'api_request_total',
    'Total API requests',
    ['endpoint', 'method', 'status']
)

active_sessions = Gauge(
    'active_sessions',
    'Number of active sessions'
)

# 中间件
@app.middleware("http")
async def monitor_performance(request: Request, call_next):
    start_time = time.time()

    # 执行请求
    response = await call_next(request)

    # 记录指标
    duration = time.time() - start_time
    api_request_duration.labels(
        endpoint=request.url.path,
        method=request.method
    ).observe(duration)

    api_request_total.labels(
        endpoint=request.url.path,
        method=request.method,
        status=response.status_code
    ).inc()

    return response
```

### 9.2 性能分析

```python
import cProfile
import pstats
from functools import wraps

def profile(func):
    """性能分析装饰器"""
    @wraps(func)
    def wrapper(*args, **kwargs):
        profiler = cProfile.Profile()
        profiler.enable()

        result = func(*args, **kwargs)

        profiler.disable()
        stats = pstats.Stats(profiler)
        stats.sort_stats('cumulative')
        stats.print_stats(20)  # 打印前 20 个最慢的函数

        return result
    return wrapper

# 使用
@profile
async def slow_function():
    # ... 代码
    pass
```

---

## 总结

性能优化关键要点：

1. **响应时间**: LLM 预热、连接复用、工具并行化
2. **并发处理**: 异步 I/O、工作池、速率限制
3. **数据库**: 索引优化、批量查询、连接池
4. **缓存**: 多层缓存、预热、智能失效
5. **网络**: 压缩、HTTP/2、CDN
6. **资源**: 内存管理、CPU 优化
7. **前端**: 代码分割、虚拟滚动、防抖节流
8. **监控**: Prometheus、性能分析

**目标**: P95 响应时间 <500ms，支持 1000+ 并发用户
