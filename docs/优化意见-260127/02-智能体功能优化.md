# MacJarvis 智能体功能优化建议

> **文档版本**: v1.0
> **创建日期**: 2026-01-27
> **优化目标**: 增强 Agent 智能化水平

---

## 目录

1. [当前 Agent 能力分析](#1-当前-agent-能力分析)
2. [记忆系统设计](#2-记忆系统设计)
3. [规划与推理能力](#3-规划与推理能力)
4. [多模态支持](#4-多模态支持)
5. [Multi-Agent 协作](#5-multi-agent-协作)
6. [工具链编排](#6-工具链编排)
7. [上下文管理优化](#7-上下文管理优化)
8. [自主学习能力](#8-自主学习能力)
9. [实施路线图](#9-实施路线图)

---

## 1. 当前 Agent 能力分析

### 1.1 现有能力

✅ **基础对话能力**
- 支持流式对话
- 多轮对话上下文
- 工具调用

✅ **工具执行能力**
- 20+ macOS 工具
- 错误处理
- 超时控制

✅ **会话管理**
- 多会话支持
- 消息历史

### 1.2 能力缺陷

⚠️ **无长期记忆**
- 会话之间无知识共享
- 无法记住用户偏好
- 无法从历史中学习

⚠️ **无规划能力**
- 无法处理复杂多步骤任务
- 无法制定执行计划
- 缺乏任务分解能力

⚠️ **单模态限制**
- 仅支持文本输入
- 无法处理图片、文件
- 无法生成可视化结果

⚠️ **上下文窗口受限**
- 无法处理超长对话
- 无智能摘要
- 无上下文压缩

⚠️ **无协作能力**
- 单一 Agent 工作
- 无法分配子任务
- 无专家 Agent 系统

---

## 2. 记忆系统设计

### 2.1 三层记忆架构

```
┌─────────────────────────────────────────────────┐
│           Short-term Memory (工作记忆)           │
│  - 当前对话上下文                                │
│  - 最近 N 轮对话                                 │
│  - Redis (TTL: 1小时)                           │
└─────────────────┬───────────────────────────────┘
                  │
                  ▼
┌─────────────────────────────────────────────────┐
│          Episodic Memory (情景记忆)              │
│  - 会话历史                                      │
│  - 工具执行记录                                  │
│  - MongoDB (永久存储)                           │
└─────────────────┬───────────────────────────────┘
                  │
                  ▼
┌─────────────────────────────────────────────────┐
│          Semantic Memory (语义记忆)              │
│  - 用户偏好                                      │
│  - 知识图谱                                      │
│  - 常见问题-答案                                 │
│  - Vector DB (Pinecone/Weaviate)               │
└─────────────────────────────────────────────────┘
```

### 2.2 短期记忆 (Short-term Memory)

**用途**: 当前对话上下文

```python
class ShortTermMemory:
    def __init__(self, redis: Redis, window_size: int = 10):
        self.redis = redis
        self.window_size = window_size

    async def add_message(
        self,
        session_id: str,
        message: dict
    ) -> None:
        """添加消息到短期记忆"""
        key = f"stm:{session_id}"

        # 使用 Redis List
        await self.redis.lpush(key, json.dumps(message))
        await self.redis.ltrim(key, 0, self.window_size - 1)
        await self.redis.expire(key, 3600)  # 1小时过期

    async def get_context(self, session_id: str) -> list[dict]:
        """获取当前上下文"""
        key = f"stm:{session_id}"
        messages = await self.redis.lrange(key, 0, -1)
        return [json.loads(msg) for msg in reversed(messages)]

    async def clear(self, session_id: str):
        """清空短期记忆"""
        await self.redis.delete(f"stm:{session_id}")
```

---

### 2.3 情景记忆 (Episodic Memory)

**用途**: 完整会话历史和重要事件

```python
class EpisodicMemory:
    def __init__(self, mongo: AsyncIOMotorClient):
        self.db = mongo.macjarvis
        self.episodes = self.db.episodes

    async def store_episode(
        self,
        user_id: str,
        session_id: str,
        episode_type: str,  # conversation, task_completion, error
        content: dict,
        metadata: dict
    ) -> str:
        """存储情景记忆"""
        episode = {
            "_id": generate_id(),
            "user_id": user_id,
            "session_id": session_id,
            "type": episode_type,
            "content": content,
            "metadata": metadata,
            "timestamp": datetime.utcnow(),
            "embedding": None  # 待生成
        }

        await self.episodes.insert_one(episode)
        return episode["_id"]

    async def recall(
        self,
        user_id: str,
        query: str,
        limit: int = 5
    ) -> list[dict]:
        """回忆相关情景（基于语义搜索）"""
        # 1. 生成查询向量
        query_embedding = await self._get_embedding(query)

        # 2. 向量搜索（使用 MongoDB Vector Search）
        pipeline = [
            {
                "$vectorSearch": {
                    "index": "episode_embedding_index",
                    "path": "embedding",
                    "queryVector": query_embedding,
                    "numCandidates": 100,
                    "limit": limit,
                    "filter": {"user_id": user_id}
                }
            },
            {
                "$project": {
                    "content": 1,
                    "metadata": 1,
                    "timestamp": 1,
                    "score": {"$meta": "vectorSearchScore"}
                }
            }
        ]

        results = await self.episodes.aggregate(pipeline).to_list(limit)
        return results

    async def _get_embedding(self, text: str) -> list[float]:
        """生成文本嵌入向量"""
        response = await openai.embeddings.create(
            model="text-embedding-3-small",
            input=text
        )
        return response.data[0].embedding
```

---

### 2.4 语义记忆 (Semantic Memory)

**用途**: 长期知识存储和用户偏好

```python
@dataclass
class UserPreference:
    user_id: str
    category: str  # ui, tools, model, workflow
    key: str
    value: Any
    confidence: float  # 0-1，置信度
    learned_from: str  # 学习来源
    updated_at: datetime

class SemanticMemory:
    def __init__(self, vector_db: VectorDB, postgres: Database):
        self.vector_db = vector_db
        self.db = postgres

    async def store_knowledge(
        self,
        user_id: str,
        knowledge: str,
        category: str,
        metadata: dict = None
    ) -> str:
        """存储知识到向量数据库"""
        # 1. 生成嵌入
        embedding = await self._get_embedding(knowledge)

        # 2. 存储到向量数据库
        knowledge_id = generate_id()
        await self.vector_db.upsert(
            collection="user_knowledge",
            id=knowledge_id,
            vector=embedding,
            metadata={
                "user_id": user_id,
                "text": knowledge,
                "category": category,
                "created_at": datetime.utcnow().isoformat(),
                **(metadata or {})
            }
        )
        return knowledge_id

    async def retrieve_knowledge(
        self,
        user_id: str,
        query: str,
        limit: int = 5
    ) -> list[dict]:
        """检索相关知识"""
        query_embedding = await self._get_embedding(query)

        results = await self.vector_db.query(
            collection="user_knowledge",
            vector=query_embedding,
            filter={"user_id": user_id},
            limit=limit,
            include_metadata=True
        )
        return results

    async def learn_preference(
        self,
        user_id: str,
        category: str,
        key: str,
        value: Any,
        source: str
    ):
        """学习用户偏好"""
        # 检查是否已存在
        existing = await self.db.execute(
            select(preferences).where(
                and_(
                    preferences.c.user_id == user_id,
                    preferences.c.category == category,
                    preferences.c.key == key
                )
            )
        )
        row = existing.fetchone()

        if row:
            # 更新置信度（增量学习）
            new_confidence = min(row.confidence + 0.1, 1.0)
            await self.db.execute(
                update(preferences)
                .where(preferences.c.id == row.id)
                .values(
                    value=value,
                    confidence=new_confidence,
                    updated_at=datetime.utcnow()
                )
            )
        else:
            # 创建新偏好
            await self.db.execute(
                insert(preferences).values(
                    user_id=user_id,
                    category=category,
                    key=key,
                    value=value,
                    confidence=0.5,
                    learned_from=source,
                    updated_at=datetime.utcnow()
                )
            )

    async def get_preferences(
        self,
        user_id: str,
        category: str | None = None
    ) -> list[UserPreference]:
        """获取用户偏好"""
        query = select(preferences).where(preferences.c.user_id == user_id)
        if category:
            query = query.where(preferences.c.category == category)

        query = query.order_by(preferences.c.confidence.desc())

        result = await self.db.execute(query)
        return [UserPreference(**row) for row in result.fetchall()]
```

---

### 2.5 记忆整合与召回

```python
class MemoryManager:
    def __init__(
        self,
        stm: ShortTermMemory,
        episodic: EpisodicMemory,
        semantic: SemanticMemory
    ):
        self.stm = stm
        self.episodic = episodic
        self.semantic = semantic

    async def build_context(
        self,
        user_id: str,
        session_id: str,
        current_query: str
    ) -> str:
        """构建增强上下文"""
        # 1. 短期记忆（当前对话）
        recent_messages = await self.stm.get_context(session_id)

        # 2. 情景记忆（相关历史）
        relevant_episodes = await self.episodic.recall(
            user_id,
            current_query,
            limit=3
        )

        # 3. 语义记忆（用户偏好和知识）
        preferences = await self.semantic.get_preferences(user_id)
        relevant_knowledge = await self.semantic.retrieve_knowledge(
            user_id,
            current_query,
            limit=5
        )

        # 4. 构建增强提示词
        context = self._build_enhanced_prompt(
            recent_messages,
            relevant_episodes,
            preferences,
            relevant_knowledge
        )

        return context

    def _build_enhanced_prompt(
        self,
        messages: list[dict],
        episodes: list[dict],
        preferences: list[UserPreference],
        knowledge: list[dict]
    ) -> str:
        """构建增强提示词"""
        prompt_parts = []

        # 用户偏好
        if preferences:
            pref_text = "\n".join([
                f"- {p.category}.{p.key}: {p.value} (confidence: {p.confidence})"
                for p in preferences if p.confidence > 0.6
            ])
            prompt_parts.append(
                f"用户偏好:\n{pref_text}\n"
            )

        # 相关知识
        if knowledge:
            knowledge_text = "\n".join([
                f"- {k['metadata']['text']}"
                for k in knowledge
            ])
            prompt_parts.append(
                f"相关知识:\n{knowledge_text}\n"
            )

        # 相关历史
        if episodes:
            episode_text = "\n".join([
                f"- [{e['timestamp'].strftime('%Y-%m-%d')}] {e['content']['summary']}"
                for e in episodes
            ])
            prompt_parts.append(
                f"相关历史:\n{episode_text}\n"
            )

        # 当前对话
        current_text = "\n".join([
            f"{m['role']}: {m['content']}"
            for m in messages
        ])
        prompt_parts.append(
            f"当前对话:\n{current_text}\n"
        )

        return "\n---\n".join(prompt_parts)
```

---

## 3. 规划与推理能力

### 3.1 ReAct 模式增强

当前 Agent 使用简单的工具调用循环，缺乏显式的推理过程。

**改进方案**: 引入 ReAct (Reasoning + Acting) 模式

```python
class ReActAgent:
    """
    ReAct 模式：思考 → 行动 → 观察 循环
    """

    def __init__(
        self,
        llm_client: LLMClient,
        tool_registry: ToolRegistry
    ):
        self.llm = llm_client
        self.tools = tool_registry

    async def run(
        self,
        user_query: str,
        max_iterations: int = 8
    ) -> str:
        """运行 ReAct 循环"""
        thought_history = []
        action_history = []

        for i in range(max_iterations):
            # 1. Thought (思考)
            thought = await self._think(
                user_query,
                thought_history,
                action_history
            )
            thought_history.append(thought)

            # 2. 判断是否完成
            if thought.get("finished"):
                return thought.get("answer")

            # 3. Action (行动)
            action = thought.get("action")
            if action:
                observation = await self._act(action)
                action_history.append({
                    "action": action,
                    "observation": observation
                })

        # 达到最大迭代次数
        return "抱歉，任务过于复杂，无法在当前迭代次数内完成。"

    async def _think(
        self,
        query: str,
        thoughts: list[dict],
        actions: list[dict]
    ) -> dict:
        """思考下一步"""
        prompt = self._build_react_prompt(query, thoughts, actions)

        response = await self.llm.chat(
            messages=[{"role": "user", "content": prompt}],
            response_format={"type": "json_object"}
        )

        return json.loads(response.content)

    async def _act(self, action: dict) -> str:
        """执行行动"""
        tool_name = action["tool"]
        tool_args = action["args"]

        result = await self.tools.execute(tool_name, tool_args)
        return json.dumps(result)

    def _build_react_prompt(
        self,
        query: str,
        thoughts: list[dict],
        actions: list[dict]
    ) -> str:
        """构建 ReAct 提示词"""
        return f"""
你是一个能够推理和行动的 AI 助手。

用户问题: {query}

可用工具:
{self._format_tools()}

思考历史:
{self._format_thoughts(thoughts)}

行动历史:
{self._format_actions(actions)}

请按照以下格式输出 JSON:
{{
    "thought": "你的思考过程",
    "finished": false,
    "action": {{
        "tool": "工具名称",
        "args": {{"参数": "值"}}
    }}
}}

如果任务完成，返回:
{{
    "thought": "总结",
    "finished": true,
    "answer": "最终答案"
}}
"""
```

---

### 3.2 任务分解与规划

对于复杂任务，需要先制定计划，再逐步执行。

```python
@dataclass
class Task:
    id: str
    description: str
    tool: str
    args: dict
    dependencies: list[str]  # 依赖的任务 ID
    status: str  # pending, running, completed, failed
    result: Any = None
    error: str | None = None

class TaskPlanner:
    """任务规划器"""

    async def create_plan(
        self,
        user_goal: str,
        available_tools: list[Tool]
    ) -> list[Task]:
        """创建任务执行计划"""
        # 使用 LLM 生成任务计划
        prompt = f"""
用户目标: {user_goal}

可用工具:
{self._format_tools(available_tools)}

请将用户目标分解为一系列可执行的子任务，输出 JSON 格式:
{{
    "tasks": [
        {{
            "id": "task_1",
            "description": "任务描述",
            "tool": "工具名称",
            "args": {{"参数": "值"}},
            "dependencies": []
        }},
        {{
            "id": "task_2",
            "description": "依赖 task_1 的任务",
            "tool": "另一个工具",
            "args": {{"参数": "值"}},
            "dependencies": ["task_1"]
        }}
    ]
}}
"""

        response = await self.llm.chat(
            messages=[{"role": "user", "content": prompt}],
            response_format={"type": "json_object"}
        )

        plan_data = json.loads(response.content)
        tasks = [
            Task(
                id=t["id"],
                description=t["description"],
                tool=t["tool"],
                args=t["args"],
                dependencies=t["dependencies"],
                status="pending"
            )
            for t in plan_data["tasks"]
        ]

        return tasks

class TaskExecutor:
    """任务执行器（支持依赖关系）"""

    async def execute_plan(
        self,
        tasks: list[Task],
        tool_registry: ToolRegistry
    ) -> dict:
        """执行任务计划"""
        task_map = {t.id: t for t in tasks}
        completed = set()

        while len(completed) < len(tasks):
            # 找到所有依赖已满足的待执行任务
            ready_tasks = [
                t for t in tasks
                if t.status == "pending" and
                all(dep in completed for dep in t.dependencies)
            ]

            if not ready_tasks:
                # 没有可执行任务，检查是否有失败任务
                failed = [t for t in tasks if t.status == "failed"]
                if failed:
                    return {
                        "ok": False,
                        "error": f"任务失败: {failed[0].error}"
                    }
                break

            # 并行执行所有就绪任务
            await asyncio.gather(*[
                self._execute_task(task, task_map, tool_registry)
                for task in ready_tasks
            ])

        return {
            "ok": True,
            "results": {t.id: t.result for t in tasks}
        }

    async def _execute_task(
        self,
        task: Task,
        task_map: dict[str, Task],
        tool_registry: ToolRegistry
    ):
        """执行单个任务"""
        task.status = "running"

        try:
            # 替换参数中的任务引用
            resolved_args = self._resolve_args(task.args, task_map)

            # 执行工具
            result = await tool_registry.execute(
                task.tool,
                resolved_args
            )

            if result.get("ok"):
                task.status = "completed"
                task.result = result.get("data")
            else:
                task.status = "failed"
                task.error = result.get("error")

        except Exception as e:
            task.status = "failed"
            task.error = str(e)

    def _resolve_args(
        self,
        args: dict,
        task_map: dict[str, Task]
    ) -> dict:
        """解析参数中的任务引用 (如 "${task_1.result}")"""
        resolved = {}
        for key, value in args.items():
            if isinstance(value, str) and value.startswith("${"):
                # 解析引用 ${task_id.result}
                ref = value[2:-1]  # 去掉 ${ 和 }
                task_id, field = ref.split(".")
                resolved[key] = getattr(task_map[task_id], field)
            else:
                resolved[key] = value
        return resolved
```

**使用示例**:

```python
# 用户: "帮我分析磁盘空间使用情况，找出占用最大的目录，并生成报告"

# 1. 生成计划
planner = TaskPlanner(llm)
tasks = await planner.create_plan(
    "分析磁盘空间，找出占用最大的目录，生成报告",
    tool_registry.get_all_tools()
)

# 生成的计划:
# tasks = [
#     Task(id="task_1", description="获取磁盘使用情况", tool="disk_usage", args={}, dependencies=[]),
#     Task(id="task_2", description="查找大文件", tool="find_large_files", args={"min_size": "100MB"}, dependencies=["task_1"]),
#     Task(id="task_3", description="生成报告", tool="generate_report", args={"data": "${task_2.result}"}, dependencies=["task_2"])
# ]

# 2. 执行计划
executor = TaskExecutor()
result = await executor.execute_plan(tasks, tool_registry)
```

---

## 4. 多模态支持

### 4.1 图像输入处理

```python
class MultiModalAgent:
    def __init__(self, llm_client: LLMClient):
        self.llm = llm_client

    async def process_with_image(
        self,
        text: str,
        image_path: str | None = None,
        image_url: str | None = None
    ) -> str:
        """处理带图片的请求"""
        content = [{"type": "text", "text": text}]

        if image_path:
            # 读取本地图片并编码为 base64
            image_data = await self._encode_image(image_path)
            content.append({
                "type": "image_url",
                "image_url": {
                    "url": f"data:image/jpeg;base64,{image_data}"
                }
            })
        elif image_url:
            content.append({
                "type": "image_url",
                "image_url": {"url": image_url}
            })

        response = await self.llm.chat(
            messages=[{"role": "user", "content": content}],
            model="gpt-4o"  # 支持视觉的模型
        )

        return response.content

    async def _encode_image(self, path: str) -> str:
        """将图片编码为 base64"""
        async with aiofiles.open(path, "rb") as f:
            image_bytes = await f.read()
        return base64.b64encode(image_bytes).decode()

# 新增工具：截图分析
@dataclass
class ScreenshotAnalysisTool:
    name: str = "analyze_screenshot"
    description: str = "分析屏幕截图内容"
    parameters: dict = field(default_factory=lambda: {
        "type": "object",
        "properties": {
            "question": {
                "type": "string",
                "description": "关于截图的问题"
            }
        },
        "required": ["question"]
    })

    async def execute(self, args: dict[str, Any]) -> dict[str, Any]:
        """执行截图分析"""
        # 1. 捕获屏幕截图
        screenshot_path = "/tmp/screenshot.png"
        subprocess.run(["screencapture", "-x", screenshot_path])

        # 2. 使用多模态模型分析
        agent = MultiModalAgent(llm_client)
        analysis = await agent.process_with_image(
            text=args["question"],
            image_path=screenshot_path
        )

        # 3. 清理临时文件
        os.remove(screenshot_path)

        return {"ok": True, "data": {"analysis": analysis}}
```

---

### 4.2 文件处理能力

```python
class DocumentProcessor:
    """文档处理器（支持 PDF, Word, Excel 等）"""

    async def extract_text(self, file_path: str) -> str:
        """提取文件文本"""
        ext = Path(file_path).suffix.lower()

        if ext == ".pdf":
            return await self._extract_pdf(file_path)
        elif ext in [".docx", ".doc"]:
            return await self._extract_word(file_path)
        elif ext in [".xlsx", ".xls"]:
            return await self._extract_excel(file_path)
        elif ext == ".txt":
            async with aiofiles.open(file_path, "r") as f:
                return await f.read()
        else:
            raise ValueError(f"Unsupported file type: {ext}")

    async def _extract_pdf(self, path: str) -> str:
        """提取 PDF 文本"""
        import PyPDF2
        text_parts = []

        async with aiofiles.open(path, "rb") as f:
            pdf_bytes = await f.read()
            reader = PyPDF2.PdfReader(io.BytesIO(pdf_bytes))

            for page in reader.pages:
                text_parts.append(page.extract_text())

        return "\n\n".join(text_parts)

    async def _extract_word(self, path: str) -> str:
        """提取 Word 文本"""
        import docx
        doc = docx.Document(path)
        return "\n\n".join([para.text for para in doc.paragraphs])

    async def _extract_excel(self, path: str) -> str:
        """提取 Excel 数据"""
        import pandas as pd
        df = pd.read_excel(path)
        return df.to_string()

# 新增工具：文档问答
@dataclass
class DocumentQATool:
    name: str = "ask_document"
    description: str = "对文档内容提问"
    parameters: dict = field(default_factory=lambda: {
        "type": "object",
        "properties": {
            "file_path": {"type": "string", "description": "文件路径"},
            "question": {"type": "string", "description": "问题"}
        },
        "required": ["file_path", "question"]
    })

    def __init__(self):
        self.processor = DocumentProcessor()
        self.llm = llm_client

    async def execute(self, args: dict[str, Any]) -> dict[str, Any]:
        """执行文档问答"""
        # 1. 提取文档内容
        text = await self.processor.extract_text(args["file_path"])

        # 2. 使用 LLM 回答问题
        prompt = f"""
文档内容:
{text[:10000]}  # 限制长度

问题: {args["question"]}

请基于文档内容回答问题。如果文档中没有相关信息，请明确说明。
"""

        response = await self.llm.chat(
            messages=[{"role": "user", "content": prompt}]
        )

        return {"ok": True, "data": {"answer": response.content}}
```

---

### 4.3 代码生成与执行

```python
@dataclass
class CodeGenerationTool:
    name: str = "generate_code"
    description: str = "生成并执行代码"
    parameters: dict = field(default_factory=lambda: {
        "type": "object",
        "properties": {
            "task": {"type": "string", "description": "编程任务描述"},
            "language": {"type": "string", "enum": ["python", "javascript", "bash"]},
            "execute": {"type": "boolean", "description": "是否执行生成的代码"}
        },
        "required": ["task", "language"]
    })

    async def execute(self, args: dict[str, Any]) -> dict[str, Any]:
        """生成代码"""
        # 1. 使用 LLM 生成代码
        prompt = f"""
请用 {args['language']} 编写代码完成以下任务:
{args['task']}

只输出代码，不要有任何解释。代码应该是可以直接运行的。
"""

        response = await llm_client.chat(
            messages=[{"role": "user", "content": prompt}]
        )

        code = response.content.strip()

        # 2. 如果需要执行
        if args.get("execute", False):
            result = await self._execute_code(code, args["language"])
            return {
                "ok": True,
                "data": {
                    "code": code,
                    "output": result
                }
            }

        return {"ok": True, "data": {"code": code}}

    async def _execute_code(self, code: str, language: str) -> str:
        """在沙箱中执行代码"""
        if language == "python":
            # 使用 Docker 隔离执行
            result = subprocess.run(
                ["docker", "run", "--rm", "-i", "python:3.12-slim", "python", "-c", code],
                capture_output=True,
                text=True,
                timeout=30
            )
            return result.stdout or result.stderr

        elif language == "javascript":
            result = subprocess.run(
                ["docker", "run", "--rm", "-i", "node:20-slim", "node", "-e", code],
                capture_output=True,
                text=True,
                timeout=30
            )
            return result.stdout or result.stderr

        elif language == "bash":
            result = subprocess.run(
                ["docker", "run", "--rm", "-i", "bash:5", "bash", "-c", code],
                capture_output=True,
                text=True,
                timeout=30
            )
            return result.stdout or result.stderr
```

---

## 5. Multi-Agent 协作

### 5.1 专家 Agent 系统

不同类型的任务由专门的 Agent 处理：

```python
@dataclass
class AgentProfile:
    name: str
    role: str
    expertise: list[str]
    tools: list[str]
    system_prompt: str

# 定义专家 Agent
EXPERT_AGENTS = {
    "system_admin": AgentProfile(
        name="SystemAdmin",
        role="系统管理专家",
        expertise=["system_info", "process_management", "performance"],
        tools=["system_info", "process_list", "top_processes", "disk_usage"],
        system_prompt="你是一位 macOS 系统管理专家，擅长系统诊断和性能优化。"
    ),
    "file_manager": AgentProfile(
        name="FileManager",
        role="文件管理专家",
        expertise=["file_operations", "search", "organization"],
        tools=["list_directory", "search_files", "read_file", "write_file"],
        system_prompt="你是一位文件管理专家，擅长文件搜索、组织和管理。"
    ),
    "network_engineer": AgentProfile(
        name="NetworkEngineer",
        role="网络工程师",
        expertise=["networking", "connectivity", "troubleshooting"],
        tools=["network_info", "open_ports", "dns_info", "wifi_info"],
        system_prompt="你是一位网络工程师，擅长网络诊断和故障排查。"
    ),
    "data_analyst": AgentProfile(
        name="DataAnalyst",
        role="数据分析师",
        expertise=["data_analysis", "visualization", "statistics"],
        tools=["generate_code", "ask_document"],
        system_prompt="你是一位数据分析师，擅长数据处理和可视化分析。"
    )
}

class AgentCoordinator:
    """Agent 协调器"""

    def __init__(self, llm_client: LLMClient, tool_registry: ToolRegistry):
        self.llm = llm_client
        self.tools = tool_registry
        self.agents = EXPERT_AGENTS

    async def route_query(self, query: str) -> str:
        """路由查询到合适的 Agent"""
        # 1. 使用 LLM 判断应该使用哪个专家
        routing_prompt = f"""
用户查询: {query}

可用专家:
{self._format_agents()}

请选择最适合处理此查询的专家，输出 JSON:
{{"agent": "agent_id", "reason": "选择理由"}}
"""

        response = await self.llm.chat(
            messages=[{"role": "user", "content": routing_prompt}],
            response_format={"type": "json_object"}
        )

        routing = json.loads(response.content)
        agent_id = routing["agent"]

        # 2. 使用选中的 Agent 处理
        return await self.process_with_agent(agent_id, query)

    async def process_with_agent(
        self,
        agent_id: str,
        query: str
    ) -> str:
        """使用指定 Agent 处理查询"""
        agent_profile = self.agents[agent_id]

        # 构建专家 Agent
        expert_agent = Agent(
            client=self.llm,
            registry=self._get_agent_tools(agent_profile),
            system_prompt=agent_profile.system_prompt
        )

        # 执行
        return await expert_agent.run(query)

    def _get_agent_tools(self, profile: AgentProfile) -> ToolRegistry:
        """获取 Agent 可用的工具子集"""
        tools = [
            tool for tool in self.tools.tools
            if tool.name in profile.tools
        ]
        return ToolRegistry(tools)

    async def collaborate(self, query: str) -> str:
        """多个 Agent 协作完成复杂任务"""
        # 1. 分解任务
        planner = TaskPlanner(self.llm)
        tasks = await planner.create_plan(query, self.tools.get_all_tools())

        # 2. 为每个任务分配 Agent
        task_assignments = []
        for task in tasks:
            # 根据工具选择 Agent
            agent_id = self._select_agent_for_tool(task.tool)
            task_assignments.append((task, agent_id))

        # 3. 并行执行任务
        results = []
        for task, agent_id in task_assignments:
            if all(dep in [t.id for t in results] for dep in task.dependencies):
                result = await self.process_with_agent(
                    agent_id,
                    task.description
                )
                results.append(result)

        # 4. 汇总结果
        return await self._summarize_results(query, results)

    def _select_agent_for_tool(self, tool_name: str) -> str:
        """根据工具选择 Agent"""
        for agent_id, profile in self.agents.items():
            if tool_name in profile.tools:
                return agent_id
        return "system_admin"  # 默认
```

---

### 5.2 Agent 通信协议

```python
@dataclass
class AgentMessage:
    from_agent: str
    to_agent: str
    message_type: str  # request, response, notification
    content: dict
    conversation_id: str
    timestamp: datetime

class AgentCommunicationBus:
    """Agent 通信总线"""

    def __init__(self, redis: Redis):
        self.redis = redis

    async def send_message(self, message: AgentMessage):
        """发送消息"""
        channel = f"agent:{message.to_agent}"
        await self.redis.publish(
            channel,
            json.dumps(asdict(message))
        )

    async def subscribe(
        self,
        agent_id: str,
        handler: Callable[[AgentMessage], None]
    ):
        """订阅消息"""
        pubsub = self.redis.pubsub()
        await pubsub.subscribe(f"agent:{agent_id}")

        async for message in pubsub.listen():
            if message["type"] == "message":
                agent_message = AgentMessage(**json.loads(message["data"]))
                await handler(agent_message)
```

---

## 6. 工具链编排

### 6.1 工具组合

自动将多个工具组合成工具链：

```python
class ToolChain:
    """工具链：多个工具的组合"""

    def __init__(self, name: str, description: str, steps: list[dict]):
        self.name = name
        self.description = description
        self.steps = steps  # [{"tool": "tool1", "args": {...}}, ...]

    async def execute(self, registry: ToolRegistry, input_data: dict) -> dict:
        """执行工具链"""
        context = {"input": input_data}

        for i, step in enumerate(self.steps):
            tool_name = step["tool"]
            args_template = step["args"]

            # 解析参数（支持引用前面步骤的结果）
            args = self._resolve_args(args_template, context)

            # 执行工具
            result = await registry.execute(tool_name, args)

            if not result.get("ok"):
                return {
                    "ok": False,
                    "error": f"Step {i+1} failed: {result.get('error')}"
                }

            # 保存结果到上下文
            context[f"step_{i+1}"] = result.get("data")

        return {"ok": True, "data": context}

    def _resolve_args(self, args_template: dict, context: dict) -> dict:
        """解析参数模板"""
        resolved = {}
        for key, value in args_template.items():
            if isinstance(value, str) and value.startswith("$"):
                # 引用上下文变量 $step_1.result
                path = value[1:].split(".")
                resolved[key] = self._get_nested(context, path)
            else:
                resolved[key] = value
        return resolved

    def _get_nested(self, data: dict, path: list[str]) -> Any:
        """获取嵌套字段"""
        for key in path:
            data = data[key]
        return data

# 预定义工具链
TOOL_CHAINS = {
    "disk_cleanup": ToolChain(
        name="disk_cleanup",
        description="分析磁盘并清理大文件",
        steps=[
            {"tool": "disk_usage", "args": {}},
            {"tool": "search_files", "args": {"pattern": "*", "min_size": "100MB"}},
            {"tool": "move_to_trash", "args": {"paths": "$step_2.files"}}
        ]
    ),
    "performance_report": ToolChain(
        name="performance_report",
        description="生成系统性能报告",
        steps=[
            {"tool": "system_info", "args": {}},
            {"tool": "top_processes", "args": {"limit": 10}},
            {"tool": "disk_usage", "args": {}},
            {"tool": "generate_report", "args": {
                "system": "$step_1",
                "processes": "$step_2",
                "disk": "$step_3"
            }}
        ]
    )
}
```

---

## 7. 上下文管理优化

### 7.1 智能摘要

当对话过长时，自动生成摘要：

```python
class ContextManager:
    """上下文管理器"""

    def __init__(
        self,
        llm_client: LLMClient,
        max_tokens: int = 8000
    ):
        self.llm = llm_client
        self.max_tokens = max_tokens

    async def compress_context(
        self,
        messages: list[dict]
    ) -> list[dict]:
        """压缩上下文"""
        # 1. 估算 token 数量
        total_tokens = self._estimate_tokens(messages)

        if total_tokens < self.max_tokens:
            return messages

        # 2. 保留最近的消息和系统消息
        system_messages = [m for m in messages if m["role"] == "system"]
        recent_messages = messages[-10:]  # 最近 10 条

        # 3. 摘要中间的消息
        middle_messages = messages[len(system_messages):-10]
        if middle_messages:
            summary = await self._summarize_messages(middle_messages)
            summary_message = {
                "role": "system",
                "content": f"[历史对话摘要]\n{summary}"
            }

            return system_messages + [summary_message] + recent_messages

        return system_messages + recent_messages

    async def _summarize_messages(self, messages: list[dict]) -> str:
        """生成消息摘要"""
        conversation = "\n".join([
            f"{m['role']}: {m['content']}"
            for m in messages
        ])

        prompt = f"""
请总结以下对话的关键信息:

{conversation}

要求:
1. 提取重要的事实和决定
2. 保留用户偏好和设置
3. 记录完成的任务
4. 简洁明了（不超过 200 字）
"""

        response = await self.llm.chat(
            messages=[{"role": "user", "content": prompt}],
            model="gpt-4o-mini"  # 使用便宜的模型
        )

        return response.content

    def _estimate_tokens(self, messages: list[dict]) -> int:
        """估算 token 数量"""
        # 简单估算：1 token ≈ 4 字符
        total_chars = sum(len(m["content"]) for m in messages)
        return total_chars // 4
```

---

### 7.2 动态上下文窗口

根据任务复杂度调整上下文窗口大小：

```python
class DynamicContextWindow:
    """动态上下文窗口"""

    def __init__(self):
        self.window_sizes = {
            "simple": 4000,    # 简单对话
            "medium": 8000,    # 中等复杂
            "complex": 16000,  # 复杂任务
            "research": 32000  # 研究和分析
        }

    async def select_window_size(
        self,
        user_query: str,
        conversation_history: list[dict]
    ) -> int:
        """选择合适的窗口大小"""
        # 1. 分析查询复杂度
        complexity = await self._analyze_complexity(user_query)

        # 2. 检查历史长度
        history_tokens = self._estimate_tokens(conversation_history)

        # 3. 选择窗口大小
        required_size = self.window_sizes[complexity]
        return max(required_size, history_tokens + 4000)

    async def _analyze_complexity(self, query: str) -> str:
        """分析查询复杂度"""
        # 简单规则
        if len(query.split()) > 100:
            return "research"
        elif any(kw in query for kw in ["分析", "报告", "总结", "比较"]):
            return "complex"
        elif any(kw in query for kw in ["帮我", "如何", "为什么"]):
            return "medium"
        else:
            return "simple"
```

---

## 8. 自主学习能力

### 8.1 从反馈中学习

```python
class FeedbackLearner:
    """反馈学习器"""

    def __init__(
        self,
        semantic_memory: SemanticMemory,
        postgres: Database
    ):
        self.memory = semantic_memory
        self.db = postgres

    async def process_feedback(
        self,
        user_id: str,
        session_id: str,
        message_id: str,
        feedback: dict  # {"rating": 1-5, "comment": "..."}
    ):
        """处理用户反馈"""
        # 1. 存储反馈
        await self.db.execute(
            insert(feedbacks).values(
                user_id=user_id,
                session_id=session_id,
                message_id=message_id,
                rating=feedback["rating"],
                comment=feedback.get("comment"),
                created_at=datetime.utcnow()
            )
        )

        # 2. 如果是负面反馈，分析原因
        if feedback["rating"] <= 2:
            await self._analyze_failure(
                user_id,
                session_id,
                message_id,
                feedback
            )

        # 3. 如果是正面反馈，强化记忆
        elif feedback["rating"] >= 4:
            await self._reinforce_success(
                user_id,
                session_id,
                message_id
            )

    async def _analyze_failure(
        self,
        user_id: str,
        session_id: str,
        message_id: str,
        feedback: dict
    ):
        """分析失败原因"""
        # 获取消息和上下文
        session = await self.db.sessions.find_one({"_id": session_id})
        message = next(
            (m for m in session["messages"] if m["_id"] == message_id),
            None
        )

        if not message:
            return

        # 使用 LLM 分析失败原因
        analysis_prompt = f"""
用户对以下回答给出了负面反馈:

用户问题: {self._get_user_message(session, message_id)}
AI 回答: {message['content']}
用户反馈: {feedback.get('comment', '无评论')}
评分: {feedback['rating']}/5

请分析:
1. 回答有什么问题？
2. 应该如何改进？
3. 用户的真实需求是什么？

输出 JSON 格式:
{{
    "issues": ["问题1", "问题2"],
    "improvements": ["改进建议1", "改进建议2"],
    "user_intent": "用户真实意图"
}}
"""

        response = await llm_client.chat(
            messages=[{"role": "user", "content": analysis_prompt}],
            response_format={"type": "json_object"}
        )

        analysis = json.loads(response.content)

        # 存储到知识库
        await self.memory.store_knowledge(
            user_id=user_id,
            knowledge=json.dumps(analysis),
            category="failure_analysis",
            metadata={
                "session_id": session_id,
                "message_id": message_id,
                "feedback_rating": feedback["rating"]
            }
        )

    async def _reinforce_success(
        self,
        user_id: str,
        session_id: str,
        message_id: str
    ):
        """强化成功经验"""
        # 获取成功的交互模式
        session = await self.db.sessions.find_one({"_id": session_id})
        message = next(
            (m for m in session["messages"] if m["_id"] == message_id),
            None
        )

        # 如果使用了工具，记录成功的工具调用模式
        if message.get("tool_calls"):
            pattern = {
                "user_query_pattern": self._get_user_message(session, message_id),
                "tool_sequence": [tc["name"] for tc in message["tool_calls"]],
                "success_rating": 5
            }

            await self.memory.store_knowledge(
                user_id=user_id,
                knowledge=json.dumps(pattern),
                category="successful_pattern",
                metadata={"session_id": session_id}
            )
```

---

### 8.2 个性化优化

```python
class PersonalizationEngine:
    """个性化引擎"""

    async def get_personalized_system_prompt(
        self,
        user_id: str,
        base_prompt: str
    ) -> str:
        """生成个性化系统提示词"""
        # 1. 获取用户偏好
        preferences = await semantic_memory.get_preferences(user_id)

        # 2. 获取成功模式
        successful_patterns = await semantic_memory.retrieve_knowledge(
            user_id,
            query="successful interaction patterns",
            limit=5
        )

        # 3. 构建个性化提示
        personalization = []

        if preferences:
            pref_text = self._format_preferences(preferences)
            personalization.append(f"用户偏好:\n{pref_text}")

        if successful_patterns:
            pattern_text = self._format_patterns(successful_patterns)
            personalization.append(f"成功经验:\n{pattern_text}")

        if personalization:
            return f"{base_prompt}\n\n{chr(10).join(personalization)}"

        return base_prompt

    def _format_preferences(self, preferences: list[UserPreference]) -> str:
        """格式化偏好"""
        return "\n".join([
            f"- {p.category}: {p.key} = {p.value}"
            for p in preferences
            if p.confidence > 0.7
        ])
```

---

## 9. 实施路线图

### 阶段一：记忆系统 (3-4 周)

- [ ] 实现短期记忆（Redis）
- [ ] 实现情景记忆（MongoDB + 向量索引）
- [ ] 实现语义记忆（Pinecone/Weaviate）
- [ ] 实现记忆整合与召回

**交付物**:
- 记忆系统设计文档
- API 实现
- 单元测试

---

### 阶段二：规划能力 (2-3 周)

- [ ] 实现 ReAct 模式
- [ ] 实现任务分解器
- [ ] 实现依赖执行器

**交付物**:
- 规划系统文档
- 示例代码
- 性能测试

---

### 阶段三：多模态 (2-3 周)

- [ ] 图像输入处理
- [ ] 文档处理器
- [ ] 代码生成工具

**交付物**:
- 多模态工具集
- 使用示例

---

### 阶段四：Multi-Agent (3-4 周)

- [ ] 专家 Agent 定义
- [ ] Agent 路由器
- [ ] Agent 通信总线
- [ ] 协作机制

**交付物**:
- Multi-Agent 架构文档
- 协作示例

---

### 阶段五：自主学习 (2-3 周)

- [ ] 反馈学习器
- [ ] 个性化引擎
- [ ] A/B 测试框架

**交付物**:
- 学习系统文档
- 效果评估报告

---

## 总结

本文档提出了 MacJarvis 智能体功能的全面升级方案，包括：

1. **三层记忆系统** - 短期、情景、语义记忆
2. **规划与推理** - ReAct 模式、任务分解
3. **多模态支持** - 图像、文档、代码
4. **Multi-Agent** - 专家系统、协作机制
5. **工具链编排** - 自动化工具组合
6. **上下文管理** - 智能摘要、动态窗口
7. **自主学习** - 反馈学习、个性化

**预计周期**: 12-17 周（3-4 个月）

---

**下一步**: 请阅读性能优化、安全性等其他文档。
